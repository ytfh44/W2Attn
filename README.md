# W2-RoPE Transformer

ä¸€ä¸ªåŸºäº PyTorch å®ç°çš„ **Wasserstein-2 Attention with RoPE** æ¶æ„ã€‚
è¯¥æ¨¡å‹å°† Transformer ä¸­çš„ä¼ ç»Ÿç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰æ›¿æ¢ä¸ºåŸºäºé«˜æ–¯åˆ†å¸ƒä¹‹é—´ **Wasserstein-2 è·ç¦»** çš„åº¦é‡ï¼Œå¹¶é’ˆå¯¹å‡å€¼æµèå…¥äº† **æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)**ã€‚

## ğŸ“– æ ¸å¿ƒåŸç†

æ¯ä¸€ä¸ª Token è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¯¹è§’é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ã€‚ç½‘ç»œç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„æ®‹å·®æµï¼š
1.  **å‡å€¼æµ (Mean Stream, $\boldsymbol{\mu}$)**: ä»£è¡¨ç‰¹å¾çš„ä¸­å¿ƒä½ç½®ã€‚
2.  **ä¸ç¡®å®šæ€§æµ (Uncertainty Stream, $\mathbf{l} = \log \boldsymbol{\Sigma}$)**: ä»£è¡¨ç‰¹å¾çš„ä¸ç¡®å®šæ€§ï¼ˆæ–¹å·®çš„å¯¹æ•°ï¼‰ã€‚

### 1. Wasserstein-2 æ³¨æ„åŠ›
æ³¨æ„åŠ›åˆ†æ•°ç”±ä¸¤ä¸ªåˆ†å¸ƒé—´çš„ $W_2^2$ è·ç¦»å†³å®šï¼š
$$
S_{m,n} = - \frac{D_{\mu}^2(m, n) + D_{\sigma}^2}{\tau}
$$
å…¶ä¸­ï¼š
-   **ä½ç½®é¡¹** $D_{\mu}^2$: é€šè¿‡ RoPE æ—‹è½¬åçš„å‘é‡æ¬§æ°è·ç¦»è®¡ç®—ã€‚
-   **å½¢æ€é¡¹** $D_{\sigma}^2$: æ ‡å‡†å·®å‘é‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚

### 2. æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
RoPE ä»…åº”ç”¨äº **å‡å€¼æµ** ($\boldsymbol{\mu}$)ï¼Œä¿æŒä¸ç¡®å®šæ€§æµçš„ä½ç½®ä¸å˜æ€§ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç»å¯¹å’Œç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼ŒåŒæ—¶å¤„ç†åˆ†å¸ƒçš„å‡ ä½•ç‰¹æ€§ã€‚

### 3. è¾“å‡ºèšåˆ
-   **å‡å€¼è¾“å‡º**: å€¼çš„åŠ æƒç®—æœ¯å¹³å‡ã€‚
-   **ä¸ç¡®å®šæ€§è¾“å‡º**: å€¼çš„åŠ æƒå‡ ä½•å¹³å‡ï¼ˆå¯¹æ•°åŸŸçš„ç®—æœ¯å¹³å‡ï¼‰ï¼Œåæ˜ äº†ä¸ç¡®å®šæ€§çš„èšåˆã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

d:/PROJECTS/W2Attn/
â”œâ”€â”€ w2_rope/
â”‚   â”œâ”€â”€ __init__.py     # Exports
â”‚   â”œâ”€â”€ attention.py    # W2Attention & StandardAttention
â”‚   â”œâ”€â”€ rope.py         # RotaryEmbedding
â”‚   â”œâ”€â”€ ffn.py          # FeedForward & RMSNorm
â”‚   â”œâ”€â”€ block.py        # W2TransformerBlock & StandardBlock
â”‚   â”œâ”€â”€ config.py       # ModelConfig
â”‚   â””â”€â”€ model.py        # LanguageModel (Unified Model Wrapper)
â”œâ”€â”€ benches/
â”‚   â”œâ”€â”€ run_benchmarks.py # Unified Benchmark Entry Point
â”‚   â”œâ”€â”€ common.py         # Shared Benchmark Utils
â”‚   â””â”€â”€ hierarchy.py      # Entailment Data Generator
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml
```

## ğŸ› ï¸ å®‰è£…

éœ€è¦å®‰è£… PyTorch, NumPy å’Œ Einopsï¼š

```bash
uv add torch numpy einops
# æˆ–è€…
pip install torch numpy einops
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

å¦‚ä½•ä½¿ç”¨ `W2TransformerBlock` æ„å»ºæ¨¡å‹ï¼š

```python
import torch
from w2_rope.block import W2TransformerBlock
from w2_rope.rope import RotaryEmbedding

# 1. é…ç½®å‚æ•°
class Config:
    hidden_size = 512
    num_attention_heads = 8
    intermediate_size = 2048
    rms_norm_eps = 1e-6

config = Config()
bs, seq_len = 2, 64

# 2. åˆå§‹åŒ–æ¨¡å—
block = W2TransformerBlock(config)
head_dim = config.hidden_size // config.num_attention_heads
rope = RotaryEmbedding(head_dim)

# 3. å‡†å¤‡è¾“å…¥ (åŒæµ)
# å‡å€¼æµ (Batch, Seq, Hidden)
mu = torch.randn(bs, seq_len, config.hidden_size)
# ä¸ç¡®å®šæ€§æµ (log sigma)
log_sigma = torch.randn(bs, seq_len, config.hidden_size)

# 4. è®¡ç®— RoPE æ—‹è½¬é¡¹
# éœ€åœ¨å¤–éƒ¨è®¡ç®—å¹¶ä¼ å…¥ï¼Œä»¥ä¾¿åœ¨å¤šå±‚é—´å…±äº«æˆ–ç¼“å­˜
cos, sin = rope(mu, seq_len=seq_len)

# 5. å‰å‘ä¼ æ’­
out_mu, out_log_sigma = block(
    hidden_states_mu=mu, 
    hidden_states_log_sigma=log_sigma, 
    rotary_emb_outputs=(cos, sin)
)

print(f"Output Mean Shape: {out_mu.shape}")       # [2, 64, 512]
print(f"Output Sigma Shape: {out_log_sigma.shape}") # [2, 64, 512]
```

## âœ… éªŒè¯

é¡¹ç›®ä¸­åŒ…å«ä¸€ä¸ªéªŒè¯è„šæœ¬ï¼Œç”¨äºæ£€æŸ¥å½¢çŠ¶æ­£ç¡®æ€§ã€å‰å‘ä¼ æ’­å’Œæ¢¯åº¦åå‘ä¼ æ’­ã€‚

```bash
python tests/verify.py
```

é¢„æœŸè¾“å‡ºï¼š
```text
Running Verification Tests...
Test 1: RoPE Shapes... PASSED
Test 2: Attention Forward... PASSED
Test 3: Block Forward... PASSED
Test 4: Gradients... PASSED
```

## ğŸ“ å®ç°ç»†èŠ‚å¤‡å¿˜

1.  **è·ç¦»è®¡ç®—ä¼˜åŒ–**:
    åœ¨è®¡ç®— $D_{\mu}^2$ æ—¶ï¼Œä½¿ç”¨äº†å±•å¼€å…¬å¼ $\|\mathbf{q}\|^2 + \|\mathbf{k}\|^2 - 2 \mathbf{q}^T \mathcal{R}\mathbf{k}$ ä»¥å……åˆ†åˆ©ç”¨çŸ©é˜µä¹˜æ³•åŠ é€Ÿã€‚
2.  **æ•°å€¼ç¨³å®šæ€§**:
    Attention åˆ†ç½²é™¤ä»¥ $\tau + \epsilon$ é˜²æ­¢é™¤é›¶é”™è¯¯ã€‚
3.  **åŒæµ FFN**:
    ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ï¼Œå‡å€¼æµå’Œä¸ç¡®å®šæ€§æµæ‹¥æœ‰ç‹¬ç«‹çš„æƒé‡å‚æ•°ï¼Œäº’å¦‚æœä¸å¹²æ‰°ã€‚

## ğŸ“Š æ€§èƒ½åˆ†æ (Performance Analysis)

åŸºäº `benches/run_benchmarks.py` çš„æµ‹è¯•ç»“æœ (2025.12):

### 1. å…³è”è®°å¿† (Associative Recall) â€”â€” å¼ºé¡¹
W2 Attention åœ¨éœ€è¦æ¨¡ç³ŠåŒ¹é…å’Œè®°å¿†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œ**å‚æ•°åˆ©ç”¨ç‡æé«˜**ã€‚

| æ¨¡å‹ | å‚æ•°é‡ | Loss | å¤‡æ³¨ |
| :--- | :--- | :--- | :--- |
| **Standard Attention** | 492k | 3.68 | Baseline |
| **W2 Attention** | **279k** | **3.45** | **æ›´å°‘å‚æ•°ï¼Œæ›´ä½ Loss** |

*   **ç»“è®º**: W2 èŠ‚çœäº† ~43% çš„å‚æ•°ï¼Œå´å–å¾—äº†æ›´å¥½çš„æ”¶æ•›æ•ˆæœã€‚

### 2. é€»è¾‘æ¨ç† (Entailment) â€”â€” å¼±é¡¹
åœ¨å±‚çº§é€»è¾‘åˆ¤æ–­ä»»åŠ¡ä¸­ï¼ŒW2 ç›®å‰ç•¥é€Šäº Standard Attentionã€‚

*   **Standard**: Accuracy 97%
*   **W2**: Accuracy 88%

*   **åŸå› æ¨æµ‹**: Wasserstein è·ç¦»çš„é«˜æ–¯å¹³æ»‘ç‰¹æ€§ï¼ˆ"Smearing"ï¼‰å¯èƒ½ä¸åˆ©äºå¤„ç†é”åˆ©çš„äºŒå…ƒé€»è¾‘è¾¹ç•Œï¼Œæˆ–è€…éœ€è¦æ›´ç²¾ç»†çš„è¶…å‚è°ƒèŠ‚ï¼ˆå¦‚å­¦ä¹ ç‡ã€åˆå§‹åŒ–ï¼‰ã€‚

### 3. å¾®åŸºå‡†æµ‹è¯• (Micro-Benchmarks) & ç¼ºç‚¹
W2 çš„ä¸»è¦ç“¶é¢ˆåœ¨äº **æ˜¾å­˜å ç”¨** å’Œ **è®¡ç®—é€Ÿåº¦**ã€‚

| å®éªŒåœºæ™¯ | W2 Loss | Std Loss | W2 æ˜¾å­˜ | Std æ˜¾å­˜ | W2 é€Ÿåº¦ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Base (Seq=128) | **6.57** | 6.94 | 1.5GB | 0.2GB | ~3x Slower |
| Long (Seq=512) | **6.94** | 6.94 | **2.9GB** | **0.2GB** | ~5x Slower |
| Deep (L=4) | **5.78** | 6.93 | 2.3GB | 0.3GB | ~3x Slower |

*   **æ˜¾å­˜ç“¶é¢ˆ**: ç”±äº Naive å®ç°ä¸­éœ€è¦æ„å»º `[B, H, S, S, D]` çš„ä¸­é—´å¼ é‡æ¥è®¡ç®—æˆå¯¹è·ç¦»ï¼ŒW2 çš„æ˜¾å­˜å¤æ‚åº¦ä¸º $O(S^2 D)$ï¼Œè€Œæ ‡å‡† Attention ä¸º $O(S^2)$ã€‚
*   **æ”¹è¿›æ–¹å‘**: å¿…é¡»å®ç°è‡ªå®šä¹‰ CUDA Kernel (Fusion)ï¼Œåœ¨ SRAM ä¸­è®¡ç®—è·ç¦»å¹¶ Reduceï¼Œé¿å…æ˜¾å¼å­˜å‚¨å·¨å¤§çš„ä¸­é—´å¼ é‡ã€‚
