# W2-RoPE Transformer

ä¸€ä¸ªåŸºäº PyTorch å®ç°çš„ **Wasserstein-2 Attention with RoPE** æ¶æ„ã€‚
è¯¥æ¨¡å‹å°† Transformer ä¸­çš„ä¼ ç»Ÿç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰æ›¿æ¢ä¸ºåŸºäºé«˜æ–¯åˆ†å¸ƒä¹‹é—´ **Wasserstein-2 è·ç¦»** çš„åº¦é‡ï¼Œå¹¶é’ˆå¯¹å‡å€¼æµèå…¥äº† **æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)**ã€‚

## ğŸ“– æ ¸å¿ƒåŸç†

æ¯ä¸€ä¸ª Token è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¯¹è§’é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ã€‚ç½‘ç»œç»´æŠ¤ä¸¤ä¸ªç‹¬ç«‹çš„æ®‹å·®æµï¼š
1.  **å‡å€¼æµ (Mean Stream, $\boldsymbol{\mu}$)**: ä»£è¡¨ç‰¹å¾çš„ä¸­å¿ƒä½ç½®ã€‚
2.  **ä¸ç¡®å®šæ€§æµ (Uncertainty Stream, $\mathbf{l} = \log \boldsymbol{\Sigma}$)**: ä»£è¡¨ç‰¹å¾çš„ä¸ç¡®å®šæ€§ï¼ˆæ–¹å·®çš„å¯¹æ•°ï¼‰ã€‚

### 1. Wasserstein-2 æ³¨æ„åŠ›
æ³¨æ„åŠ›åˆ†æ•°ç”±ä¸¤ä¸ªåˆ†å¸ƒé—´çš„ $W_2^2$ è·ç¦»å†³å®šï¼š
$$
S_{m,n} = - \frac{D_{\mu}^2(m, n) + D_{\sigma}^2}{\tau}
$$
å…¶ä¸­ï¼š
-   **ä½ç½®é¡¹** $D_{\mu}^2$: é€šè¿‡ RoPE æ—‹è½¬åçš„å‘é‡æ¬§æ°è·ç¦»è®¡ç®—ã€‚
-   **å½¢æ€é¡¹** $D_{\sigma}^2$: æ ‡å‡†å·®å‘é‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚

### 2. æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
RoPE ä»…åº”ç”¨äº **å‡å€¼æµ** ($\boldsymbol{\mu}$)ï¼Œä¿æŒä¸ç¡®å®šæ€§æµçš„ä½ç½®ä¸å˜æ€§ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç»å¯¹å’Œç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼ŒåŒæ—¶å¤„ç†åˆ†å¸ƒçš„å‡ ä½•ç‰¹æ€§ã€‚

### 3. è¾“å‡ºèšåˆ
-   **å‡å€¼è¾“å‡º**: å€¼çš„åŠ æƒç®—æœ¯å¹³å‡ã€‚
-   **ä¸ç¡®å®šæ€§è¾“å‡º**: å€¼çš„åŠ æƒå‡ ä½•å¹³å‡ï¼ˆå¯¹æ•°åŸŸçš„ç®—æœ¯å¹³å‡ï¼‰ï¼Œåæ˜ äº†ä¸ç¡®å®šæ€§çš„èšåˆã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
d:/PROJECTS/W2Attn/
â”œâ”€â”€ w2_rope/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention.py    # W2Attention æ ¸å¿ƒå®ç° (W2 è·ç¦»è®¡ç®—, åŒæµæŠ•å½±)
â”‚   â”œâ”€â”€ rope.py         # RotaryEmbedding å®ç° (ä»…ä½œç”¨äºå‡å€¼)
â”‚   â”œâ”€â”€ ffn.py          # W2FeedForward (åŒæµ SwiGLU) å’Œ RMSNorm
â”‚   â””â”€â”€ block.py        # W2TransformerBlock (å®Œæ•´çš„ Transformer å—)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ verify.py       # æ‰‹åŠ¨éªŒè¯è„šæœ¬ (Shapes, Forward, Gradients)
â”‚   â””â”€â”€ test_components.py # Pytest æµ‹è¯•ç”¨ä¾‹
â”œâ”€â”€ README.md           # é¡¹ç›®æ–‡æ¡£
â””â”€â”€ pyproject.toml
```

## ğŸ› ï¸ å®‰è£…

éœ€è¦å®‰è£… PyTorch, NumPy å’Œ Einopsï¼š

```bash
uv add torch numpy einops
# æˆ–è€…
pip install torch numpy einops
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

å¦‚ä½•ä½¿ç”¨ `W2TransformerBlock` æ„å»ºæ¨¡å‹ï¼š

```python
import torch
from w2_rope.block import W2TransformerBlock
from w2_rope.rope import RotaryEmbedding

# 1. é…ç½®å‚æ•°
class Config:
    hidden_size = 512
    num_attention_heads = 8
    intermediate_size = 2048
    rms_norm_eps = 1e-6

config = Config()
bs, seq_len = 2, 64

# 2. åˆå§‹åŒ–æ¨¡å—
block = W2TransformerBlock(config)
head_dim = config.hidden_size // config.num_attention_heads
rope = RotaryEmbedding(head_dim)

# 3. å‡†å¤‡è¾“å…¥ (åŒæµ)
# å‡å€¼æµ (Batch, Seq, Hidden)
mu = torch.randn(bs, seq_len, config.hidden_size)
# ä¸ç¡®å®šæ€§æµ (log sigma)
log_sigma = torch.randn(bs, seq_len, config.hidden_size)

# 4. è®¡ç®— RoPE æ—‹è½¬é¡¹
# éœ€åœ¨å¤–éƒ¨è®¡ç®—å¹¶ä¼ å…¥ï¼Œä»¥ä¾¿åœ¨å¤šå±‚é—´å…±äº«æˆ–ç¼“å­˜
cos, sin = rope(mu, seq_len=seq_len)

# 5. å‰å‘ä¼ æ’­
out_mu, out_log_sigma = block(
    hidden_states_mu=mu, 
    hidden_states_log_sigma=log_sigma, 
    rotary_emb_outputs=(cos, sin)
)

print(f"Output Mean Shape: {out_mu.shape}")       # [2, 64, 512]
print(f"Output Sigma Shape: {out_log_sigma.shape}") # [2, 64, 512]
```

## âœ… éªŒè¯

é¡¹ç›®ä¸­åŒ…å«ä¸€ä¸ªéªŒè¯è„šæœ¬ï¼Œç”¨äºæ£€æŸ¥å½¢çŠ¶æ­£ç¡®æ€§ã€å‰å‘ä¼ æ’­å’Œæ¢¯åº¦åå‘ä¼ æ’­ã€‚

```bash
python tests/verify.py
```

é¢„æœŸè¾“å‡ºï¼š
```text
Running Verification Tests...
Test 1: RoPE Shapes... PASSED
Test 2: Attention Forward... PASSED
Test 3: Block Forward... PASSED
Test 4: Gradients... PASSED
```

## ğŸ“ å®ç°ç»†èŠ‚å¤‡å¿˜

1.  **è·ç¦»è®¡ç®—ä¼˜åŒ–**:
    åœ¨è®¡ç®— $D_{\mu}^2$ æ—¶ï¼Œä½¿ç”¨äº†å±•å¼€å…¬å¼ $\|\mathbf{q}\|^2 + \|\mathbf{k}\|^2 - 2 \mathbf{q}^T \mathcal{R}\mathbf{k}$ ä»¥å……åˆ†åˆ©ç”¨çŸ©é˜µä¹˜æ³•åŠ é€Ÿã€‚
2.  **æ•°å€¼ç¨³å®šæ€§**:
    Attention åˆ†ç½²é™¤ä»¥ $\tau + \epsilon$ é˜²æ­¢é™¤é›¶é”™è¯¯ã€‚
3.  **åŒæµ FFN**:
    ä½¿ç”¨ SwiGLU æ¿€æ´»å‡½æ•°ï¼Œå‡å€¼æµå’Œä¸ç¡®å®šæ€§æµæ‹¥æœ‰ç‹¬ç«‹çš„æƒé‡å‚æ•°ï¼Œäº’å¦‚æœä¸å¹²æ‰°ã€‚
