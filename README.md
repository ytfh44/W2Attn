# W2-RoPE Transformer

ä¸€ä¸ªåŸºäº PyTorch å®ç°çš„ **Wasserstein-2 Attention with RoPE** æ¶æ„ã€‚
è¯¥æ¨¡å‹å°† Transformer ä¸­çš„ä¼ ç»Ÿç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰æ›¿æ¢ä¸ºåŸºäºé«˜æ–¯åˆ†å¸ƒä¹‹é—´ **Wasserstein-2 è·ç¦»** çš„åº¦é‡ï¼Œå¹¶é’ˆå¯¹å‡å€¼æµèå…¥äº† **æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)**ã€‚

## ğŸ“– æ ¸å¿ƒåŸç†

è™½ç„¶åœ¨æ³¨æ„åŠ›æœºåˆ¶å†…éƒ¨æ¯ä¸€ä¸ª Token è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¯¹è§’é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ï¼Œä½†ä¸ºäº†å…¼å®¹æ ‡å‡† Transformer æ¶æ„å¹¶èŠ‚çœæ˜¾å­˜ï¼Œæœ¬å®ç°é‡‡ç”¨ **ç»Ÿä¸€æ®‹å·®æµ (Unified Residual Stream)** è®¾è®¡ï¼š

1.  **ç»Ÿä¸€æµ**: ç½‘ç»œåœ¨å±‚ä¸å±‚ä¹‹é—´ä¼ é€’å•ä¸€çš„éšè—çŠ¶æ€å‘é‡ $\mathbf{h} \in \mathbb{R}^d$ã€‚
2.  **å†…éƒ¨æŠ•å½±**: åœ¨ W2 Attention å±‚å†…éƒ¨ï¼Œéšè—çŠ¶æ€è¢«æŠ•å½±å¹¶åˆ‡åˆ†ä¸º **å‡å€¼ ($\boldsymbol{\mu}$)** å’Œ **ä¸ç¡®å®šæ€§ ($\boldsymbol{\sigma}$)** åˆ†é‡ã€‚
3.  **W2 æ³¨æ„åŠ›è®¡ç®—**:
    æ³¨æ„åŠ›åˆ†æ•°ç”±ä¸¤ä¸ªåˆ†å¸ƒé—´çš„ $W_2^2$ è·ç¦»å†³å®šï¼š
    $$
    S_{m,n} = - \frac{D_{\mu}^2(m, n) + D_{\sigma}^2}{\tau}
    $$
    å…¶ä¸­ï¼š
    -   **ä½ç½®é¡¹** $D_{\mu}^2$: é€šè¿‡ RoPE æ—‹è½¬åçš„å‘é‡æ¬§æ°è·ç¦»è®¡ç®—ã€‚
    -   **å½¢æ€é¡¹** $D_{\sigma}^2$: æ ‡å‡†å·®å‘é‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚

### æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
RoPE ä»…åº”ç”¨äºæ³¨æ„åŠ›å±‚å†…éƒ¨çš„ **å‡å€¼åˆ†é‡** ($\boldsymbol{\mu}$)ï¼Œä¿æŒä¸ç¡®å®šæ€§åˆ†é‡çš„ä½ç½®ä¸å˜æ€§ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰ç»å¯¹å’Œç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
W2Attn/
â”œâ”€â”€ w2_rope/
â”‚   â”œâ”€â”€ __init__.py     # Exports
â”‚   â”œâ”€â”€ attention.py    # W2Attention & StandardAttention
â”‚   â”œâ”€â”€ rope.py         # RotaryEmbedding
â”‚   â”œâ”€â”€ ffn.py          # FeedForward & RMSNorm
â”‚   â”œâ”€â”€ block.py        # W2TransformerBlock & StandardBlock
â”‚   â”œâ”€â”€ config.py       # ModelConfig
â”‚   â””â”€â”€ model.py        # LanguageModel (Unified Model Wrapper)
â”œâ”€â”€ benches/
â”‚   â”œâ”€â”€ run_benchmarks.py # Unified Benchmark Entry Point
â”‚   â”œâ”€â”€ common.py         # Shared Benchmark Utils
â”‚   â””â”€â”€ hierarchy.py      # Entailment Data Generator
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_components.py
â”‚   â””â”€â”€ verify.py
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml
```

## ğŸ› ï¸ å®‰è£…

éœ€è¦å®‰è£… PyTorch, NumPy å’Œ Einopsï¼š

```bash
uv add torch numpy einops
# æˆ–è€…
pip install torch numpy einops
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

å¦‚ä½•ä½¿ç”¨ `W2TransformerBlock` æ„å»ºæ¨¡å‹ï¼ˆæ³¨æ„ï¼šæ¥å£é‡‡ç”¨ç»Ÿä¸€æµè¾“å…¥ï¼‰ï¼š

```python
import torch
from w2_rope.block import W2TransformerBlock
from w2_rope.rope import RotaryEmbedding

# 1. é…ç½®å‚æ•°
class Config:
    hidden_size = 512
    num_attention_heads = 8
    intermediate_size = 2048
    rms_norm_eps = 1e-6

config = Config()
bs, seq_len = 2, 64

# 2. åˆå§‹åŒ–æ¨¡å—
block = W2TransformerBlock(config)
head_dim = config.hidden_size // config.num_attention_heads
rope = RotaryEmbedding(head_dim)

# 3. å‡†å¤‡è¾“å…¥ (ç»Ÿä¸€æµ)
# Hidden States (Batch, Seq, Hidden)
hidden_states = torch.randn(bs, seq_len, config.hidden_size)

# 4. è®¡ç®— RoPE æ—‹è½¬é¡¹
# éœ€åœ¨å¤–éƒ¨è®¡ç®—å¹¶ä¼ å…¥ï¼Œä»¥ä¾¿åœ¨å¤šå±‚é—´å…±äº«æˆ–ç¼“å­˜
# RoPE ä»…éœ€åŸºäºåºåˆ—é•¿åº¦è®¡ç®—ä¸€æ¬¡
cos, sin = rope(hidden_states, seq_len=seq_len)

# 5. å‰å‘ä¼ æ’­
out_hidden = block(
    hidden_states=hidden_states, 
    rotary_emb_outputs=(cos, sin)
)

print(f"Output Shape: {out_hidden.shape}")       # [2, 64, 512]
```

## âœ… éªŒè¯

é¡¹ç›®ä¸­åŒ…å«ä¸€ä¸ªéªŒè¯è„šæœ¬ï¼Œç”¨äºæ£€æŸ¥å½¢çŠ¶æ­£ç¡®æ€§ã€å‰å‘ä¼ æ’­å’Œæ¢¯åº¦åå‘ä¼ æ’­ã€‚

```bash
python tests/verify.py
```

é¢„æœŸè¾“å‡ºï¼š
```text
Running Verification Tests...
Test 1: RoPE Shapes... PASSED
Test 2: Attention Forward... PASSED
Test 3: Block Forward... PASSED
Test 4: Gradients... PASSED
```

## ğŸ“ å®ç°ç»†èŠ‚å¤‡å¿˜

1.  **è·ç¦»è®¡ç®—ä¼˜åŒ–**:
    åœ¨è®¡ç®— $D_{\mu}^2$ æ—¶ï¼Œä½¿ç”¨äº†å±•å¼€å…¬å¼ $\|\mathbf{q}\|^2 + \|\mathbf{k}\|^2 - 2 \mathbf{q}^T \mathcal{R}\mathbf{k}$ ä»¥å……åˆ†åˆ©ç”¨çŸ©é˜µä¹˜æ³•åŠ é€Ÿã€‚
2.  **æ•°å€¼ç¨³å®šæ€§**:
    Attention åˆ†æ•°é™¤ä»¥ $\tau + \epsilon$ é˜²æ­¢é™¤é›¶é”™è¯¯ã€‚
    Sigma æ¿€æ´»ä½¿ç”¨äº† `Softplus` ä»¥ä¿è¯éè´Ÿæ€§ã€‚
3.  **FFN**:
    ä½¿ç”¨æ ‡å‡†çš„ SwiGLU FFN å¤„ç†ç»Ÿä¸€çš„éšè—çŠ¶æ€ã€‚å‡å€¼å’Œä¸ç¡®å®šæ€§çš„äº¤äº’å‘ç”Ÿåœ¨è‡ªæ³¨æ„åŠ›å±‚çš„æ··åˆè¿‡ç¨‹ä¸­ï¼Œéšåè¢«æŠ•å½±å›ç»Ÿä¸€æµã€‚

## ğŸ“Š æ€§èƒ½åˆ†æ (Performance Analysis)

åŸºäº `benches/run_benchmarks.py` çš„æµ‹è¯•ç»“æœ (2025.12):

### 1. å…³è”è®°å¿† (Associative Recall) â€”â€” å¼ºé¡¹
W2 Attention åœ¨éœ€è¦æ¨¡ç³ŠåŒ¹é…å’Œè®°å¿†çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œ**å‚æ•°åˆ©ç”¨ç‡æé«˜**ã€‚

| æ¨¡å‹ | å‚æ•°é‡ | Loss | å¤‡æ³¨ |
| :--- | :--- | :--- | :--- |
| **Standard Attention** | 492k | 3.68 | Baseline |
| **W2 Attention** | **279k** | **3.45** | **æ›´å°‘å‚æ•°ï¼Œæ›´ä½ Loss** |

*   **ç»“è®º**: W2 èŠ‚çœäº† ~43% çš„å‚æ•°ï¼Œå´å–å¾—äº†æ›´å¥½çš„æ”¶æ•›æ•ˆæœã€‚

### 2. é€»è¾‘æ¨ç† (Entailment) â€”â€” æ”¹è¿›
ä¼˜åŒ–åçš„ W2 Attention åœ¨é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸ Standard Attention æŒå¹³ï¼Œèƒ½å¤Ÿè¾¾åˆ° 100% å‡†ç¡®ç‡ã€‚

*   **Standard**: Accuracy 100%, Loss 0.0078
*   **W2**: **Accuracy 100%, Loss 0.0067**

*   **ç»“è®º**: é‡‡ç”¨ Scalar Sigma è¿‘ä¼¼åï¼Œæ¨¡å‹ä¸ä»…å¤§å¹…èŠ‚çœæ˜¾å­˜ï¼Œä¸”ä¿ç•™äº†åŸæœ¬çš„å­¦ä¹ èƒ½åŠ›ã€‚

### 3. å¾®åŸºå‡†æµ‹è¯• (Micro-Benchmarks) â€”â€” æ˜¾å­˜ä¼˜åŒ–
ç»è¿‡ `dist_sq` å±•å¼€ä¸ Scalar Sigma ä¼˜åŒ–åï¼ŒW2 Attention çš„ **æ˜¾å­˜å ç”¨** å·²å¤§å¹…é™ä½ï¼Œå½»åº•æ¶ˆé™¤äº†æ˜¾å­˜çˆ†ç‚¸é—®é¢˜ã€‚

| å®éªŒåœºæ™¯ | W2 Loss | Std Loss | W2 æ˜¾å­˜ | Std æ˜¾å­˜ | W2 é€Ÿåº¦ |
| :--- | :--- | :--- | :--- | :--- | :--- |
| Base (Seq=128) | **6.80** | 6.94 | **198MB** | 192MB | ~1.2x Slower |
| Long (Seq=512) | **6.94** | 6.96 | **227MB** | 176MB | ~1.2x Slower |
| Deep (L=4) | **5.84** | 6.49 | **311MB** | 300MB | ~1.2x Slower |

*   **æ˜¾å­˜ä¼˜åŒ–**: æ˜¾å­˜å ç”¨ä»åŸå…ˆçš„ GB çº§åˆ«ï¼ˆå¦‚ Long åœºæ™¯ 2.9GBï¼‰é™ä½è‡³ MB çº§åˆ«ï¼ˆ227MBï¼‰ï¼Œä¸ Standard Attention å‡ ä¹æŒå¹³ã€‚
*   **é€Ÿåº¦**: è™½ç„¶å¼•å…¥äº†é¢å¤–çš„ log å’Œ exp è®¡ç®—ï¼Œä½†é¿å…äº†å¤§å¼ é‡è¯»å†™ï¼Œé€Ÿåº¦ä¸ Standard Attention ç›¸å½“ã€‚
*   **å®ç°**: ä½¿ç”¨äº† $\mathbf{q}^2 + \mathbf{k}^2 - 2\mathbf{q}\mathbf{k}^T$ å±•å¼€å’Œ `sigma` æ ‡é‡è¿‘ä¼¼ï¼Œå°†å¤æ‚åº¦ä» $O(S^2 D)$ é™ä½ä¸º $O(S^2)$ã€‚
