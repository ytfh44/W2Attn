# W2Attn

[English](README_EN.md)

ä¸€ä¸ªåŸºäº PyTorch å®ç°çš„ **Wasserstein-2 Attention**ã€‚
æœ¬é¡¹ç›®å°† Transformer ä¸­çš„ä¼ ç»Ÿç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰æ›¿æ¢ä¸ºåŸºäºé«˜æ–¯åˆ†å¸ƒä¹‹é—´ **Wasserstein-2 è·ç¦»** çš„åº¦é‡ã€‚

## ğŸ“– æ ¸å¿ƒåŸç†

åœ¨ W2 Attention ä¸­ï¼Œæ¯ä¸€ä¸ª Token è¢«å»ºæ¨¡ä¸ºä¸€ä¸ªå¯¹è§’é«˜æ–¯åˆ†å¸ƒ $\mathcal{N}(\boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2))$ã€‚ä¸ºäº†å…¼å®¹æ ‡å‡† Transformer æ¶æ„å¹¶èŠ‚çœæ˜¾å­˜ï¼Œæœ¬å®ç°é‡‡ç”¨ **ç»Ÿä¸€æ®‹å·®æµ (Unified Residual Stream)** è®¾è®¡ï¼š

1.  **ç»Ÿä¸€æµ**: ç½‘ç»œåœ¨å±‚ä¸å±‚ä¹‹é—´ä¼ é€’å•ä¸€çš„éšè—çŠ¶æ€å‘é‡ $\mathbf{h} \in \mathbb{R}^d$ã€‚
2.  **å†…éƒ¨æŠ•å½±**: åœ¨ W2 Attention å±‚å†…éƒ¨ï¼Œéšè—çŠ¶æ€è¢«æŠ•å½±å¹¶åˆ‡åˆ†ä¸º **å‡å€¼ ($\boldsymbol{\mu}$)** å’Œ **ä¸ç¡®å®šæ€§ ($\boldsymbol{\sigma}$)** åˆ†é‡ã€‚
3.  **W2 æ³¨æ„åŠ›è®¡ç®—**:
    æ³¨æ„åŠ›åˆ†æ•°ç”±ä¸¤ä¸ªåˆ†å¸ƒé—´çš„ $W_2^2$ è·ç¦»å†³å®šï¼š
    $$
    S_{m,n} = - \frac{D_{\mu}^2(m, n) + D_{\sigma}^2}{\tau}
    $$
    å…¶ä¸­ï¼š
    -   $D_{\mu}^2$: å‡å€¼å‘é‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ï¼ˆåŒ…å«ä½ç½®ä¿¡æ¯ï¼‰ã€‚
    -   $D_{\sigma}^2$: æ ‡å‡†å·®å‘é‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚

**å…³äºä½ç½®ç¼–ç **: æœ¬å®ç°å°† **æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)** åº”ç”¨äºæ³¨æ„åŠ›å±‚å†…éƒ¨çš„å‡å€¼åˆ†é‡ ($\boldsymbol{\mu}$)ï¼Œä»¥æ•æ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
W2Attn/
â”œâ”€â”€ w2attn/           # æ ¸å¿ƒåŒ…
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ attention.py    # W2Attention é€»è¾‘
â”‚   â”œâ”€â”€ block.py        # W2TransformerBlock
â”‚   â”œâ”€â”€ model.py        # LanguageModel åŒ…è£…å™¨
â”‚   â”œâ”€â”€ config.py       # é…ç½®
â”‚   â”œâ”€â”€ ffn.py          # FFN & RMSNorm
â”‚   â””â”€â”€ rope.py         # RotaryEmbedding å®ç°
â”œâ”€â”€ benches/          # åŸºå‡†æµ‹è¯•
â”œâ”€â”€ tests/            # æµ‹è¯•
â”œâ”€â”€ README.md
â””â”€â”€ pyproject.toml
```

## ğŸ› ï¸ å®‰è£…

éœ€è¦å®‰è£… PyTorch, NumPy å’Œ Einopsï¼š

```bash
uv add torch numpy einops
# æˆ–è€…
pip install torch numpy einops
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

ä½¿ç”¨ `W2TransformerBlock` æ„å»ºæ¨¡å‹ï¼š

```python
import torch
from w2attn.block import W2TransformerBlock
from w2attn.rope import RotaryEmbedding

# 1. é…ç½®å‚æ•°
class Config:
    hidden_size = 512
    num_attention_heads = 8
    intermediate_size = 2048
    rms_norm_eps = 1e-6

config = Config()
bs, seq_len = 2, 64

# 2. åˆå§‹åŒ–æ¨¡å—
block = W2TransformerBlock(config)
head_dim = config.hidden_size // config.num_attention_heads
rope = RotaryEmbedding(head_dim)

# 3. è¾“å…¥ (ç»Ÿä¸€æµ)
# Hidden States (Batch, Seq, Hidden)
hidden_states = torch.randn(bs, seq_len, config.hidden_size)

# 4. è®¡ç®— RoPE (å¯é€‰å¤–éƒ¨è®¡ç®—ä»¥ç¼“å­˜)
cos, sin = rope(hidden_states, seq_len=seq_len)

# 5. å‰å‘ä¼ æ’­
out_hidden = block(
    hidden_states=hidden_states, 
    rotary_emb_outputs=(cos, sin)
)

print(f"Output Shape: {out_hidden.shape}")       # [2, 64, 512]
```

## âœ… éªŒè¯

è¿è¡ŒéªŒè¯è„šæœ¬ä»¥æ£€æŸ¥å½¢çŠ¶ã€å‰å‘ä¼ æ’­å’Œæ¢¯åº¦ï¼š

```bash
python tests/verify.py
```

é¢„æœŸè¾“å‡ºï¼š
```text
Running Verification Tests...
Test 1: RoPE Shapes... PASSED
Test 2: Attention Forward... PASSED
Test 3: Block Forward... PASSED
Test 4: Gradients... PASSED
```

## ğŸ“ å®ç°ç»†èŠ‚å¤‡å¿˜

1.  **è·ç¦»è®¡ç®—ä¼˜åŒ–**:
    åœ¨è®¡ç®— $D_{\mu}^2$ æ—¶ï¼Œä½¿ç”¨äº†å±•å¼€å…¬å¼ $\|\mathbf{q}\|^2 + \|\mathbf{k}\|^2 - 2 \mathbf{q}^T \mathcal{R}\mathbf{k}$ ä»¥åˆ©ç”¨çŸ©é˜µä¹˜æ³•ã€‚
2.  **æ•°å€¼ç¨³å®šæ€§**:
    Attention åˆ†æ•°é™¤ä»¥ $\tau + \epsilon$ é˜²æ­¢é™¤é›¶é”™è¯¯ã€‚Sigma ä½¿ç”¨ `Softplus` æ¿€æ´»ä¿è¯éè´Ÿæ€§ã€‚
3.  **æ¶æ„è®¾è®¡**:
    å‡å€¼å’Œä¸ç¡®å®šæ€§çš„äº¤äº’ä»…å‘ç”Ÿåœ¨è‡ªæ³¨æ„åŠ›å±‚çš„æ··åˆè¿‡ç¨‹ä¸­ï¼Œéšåè¢«æŠ•å½±å›ç»Ÿä¸€æµã€‚

## ğŸ“Š æ€§èƒ½åˆ†æ

åŸºå‡†æµ‹è¯• (2025.12):

### 1. å…³è”è®°å¿† (Associative Recall)
W2 Attention åœ¨éœ€è¦æ¨¡ç³ŠåŒ¹é…çš„ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæ˜¾ç¤ºå‡º**æé«˜çš„å‚æ•°æ•ˆç‡**ã€‚
*   **ç»“æœ**: ç›¸æ¯” Standard Attentionï¼ŒW2 èŠ‚çœäº† ~43% çš„å‚æ•°ï¼Œå¹¶å…·æœ‰æ›´å¥½çš„æ”¶æ•›æ€§ (Loss 3.45 vs 3.68)ã€‚

### 2. å¾®åŸºå‡†æµ‹è¯• (æ˜¾å­˜)
é€šè¿‡ **Diagonal Sigma** ä¼˜åŒ–ï¼Œæ˜¾å­˜å ç”¨ä¸ Standard Attention ç›¸å½“ã€‚
*   **æ˜¾å­˜**: ä» GB çº§åˆ«å¤§å¹…é™ä½è‡³ MB çº§åˆ« (ä¾‹å¦‚ Seq=512 æ—¶ä¸º 227MB)ï¼Œæœ‰æ•ˆåŒ¹é… Standard Attentionã€‚
*   **é€Ÿåº¦**: æ¯” Standard Attention æ…¢ ~1.2xï¼ˆç”±äºé¢å¤–çš„ log/exp æ“ä½œï¼‰ï¼Œä½†å¤æ‚åº¦å¢é•¿ä»ä¿æŒä¸º $O(S^2)$ã€‚
